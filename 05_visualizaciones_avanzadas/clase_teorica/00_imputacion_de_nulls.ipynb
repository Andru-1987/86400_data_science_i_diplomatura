{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20893c23",
   "metadata": {},
   "source": [
    "# (Extra) Imputacion de data y analisis de Valores Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f220dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cc7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_DATASET:str = r\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2936bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_columns:list[str] = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "columns_clean:list[str] = map( lambda x: x.lower(), _columns)\n",
    "\n",
    "df = pd.read_csv(URL_DATASET, header=None, sep=\",\",names=list(columns_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(0,np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.isna().mean() * 100).round(2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5270c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db0c60",
   "metadata": {},
   "source": [
    "## Analisiside cantidad de nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921f922",
   "metadata": {},
   "source": [
    "## Imputacion por la mediana\n",
    "- Mediana vs Media(Promedio)\n",
    "\n",
    "Para un proyecto de machine learning, la elecci√≥n entre media, moda y mediana depende del tipo de datos y del contexto espec√≠fico. Te explico cu√°ndo usar cada una:\n",
    "\n",
    "## **Media (Promedio)**\n",
    "**Cu√°ndo usarla:**\n",
    "- Con datos num√©ricos continuos sin valores at√≠picos extremos\n",
    "- Cuando la distribuci√≥n es aproximadamente normal\n",
    "- Para variables como edad, ingresos, temperatura, etc. (sin outliers)\n",
    "\n",
    "**Ejemplo:** Si tienes edades como [25, 28, 30, 32, 35], la media (30) representa bien el conjunto.\n",
    "\n",
    "## **Mediana**\n",
    "**Cu√°ndo usarla:**\n",
    "- Con datos num√©ricos que tienen valores at√≠picos (outliers)\n",
    "- Cuando la distribuci√≥n es sesgada\n",
    "- Para datos ordinales\n",
    "- Es m√°s robusta ante valores extremos\n",
    "\n",
    "**Ejemplo:** En ingresos como [30K, 35K, 40K, 45K, 500K], la mediana (40K) es m√°s representativa que la media (130K).\n",
    "\n",
    "## **Moda**\n",
    "**Cu√°ndo usarla:**\n",
    "- Con datos categ√≥ricos (texto, etiquetas)\n",
    "- Con datos num√©ricos discretos donde buscas el valor m√°s frecuente\n",
    "- Para variables como g√©nero, pa√≠s, categor√≠a de producto, etc.\n",
    "\n",
    "**Ejemplo:** En una columna de pa√≠ses [M√©xico, Argentina, M√©xico, Brasil, M√©xico], la moda es \"M√©xico\".\n",
    "\n",
    "## **Recomendaciones pr√°cticas:**\n",
    "\n",
    "1. **Analiza primero la distribuci√≥n** de tus datos con histogramas o boxplots\n",
    "2. **Detecta outliers** antes de decidir\n",
    "3. **Considera el contexto del negocio** - ¬øqu√© medida tiene m√°s sentido para tu problema?\n",
    "4. **Prueba diferentes enfoques** y eval√∫a el impacto en tu modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generar_distribuciones(df: pd.DataFrame) -> None:\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    df.hist(bins=20, figsize=(16, 9), layout=(3, 3), color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    plt.suptitle('Distribuciones ', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa655e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "medianas = df.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bada4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputada_mediana = df.fillna(medianas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32459ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generar_distribuciones(df_imputada_mediana)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95dcb3d",
   "metadata": {},
   "source": [
    "## Usando tecnicas mas eficientes dado a la cantidad de datos nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87dbafc",
   "metadata": {},
   "source": [
    "## Missing Data Imputation\n",
    "\n",
    "Cuando tienes valores nulos (`NaN`) en un dataset, la **imputaci√≥n** busca reemplazarlos con estimaciones razonables para que los modelos de machine learning o an√°lisis estad√≠sticos no se rompan ni sesguen.\n",
    "\n",
    "El **objetivo** es hacerlo de una forma que mantenga las **relaciones (correlaciones)** entre variables y **minimice el sesgo**.\n",
    "\n",
    "---\n",
    "\n",
    "## M√©todos comunes de imputaci√≥n en Scikit-Learn\n",
    "\n",
    "### A. `SimpleImputer`\n",
    "\n",
    "**Idea:** reemplaza los valores faltantes con una **constante**, **media**, **mediana** o **moda** de la columna.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"edad\": [25, np.nan, 30, 40],\n",
    "    \"ingreso\": [50000, 60000, np.nan, 80000]\n",
    "})\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "```\n",
    "\n",
    "* **Ventajas:** simple, r√°pido, √∫til cuando los datos son poco correlacionados.\n",
    "* **Desventajas:** ignora la relaci√≥n entre columnas; cada columna se imputa de forma independiente.\n",
    "\n",
    ">  Si tus variables est√°n correlacionadas (por ejemplo, edad e ingreso), este m√©todo puede introducir sesgos.\n",
    "\n",
    "---\n",
    "\n",
    "### B. `KNNImputer`\n",
    "\n",
    "**Idea:** usa los **vecinos m√°s cercanos** (por distancia eucl√≠dea entre filas completas) para imputar los valores faltantes.\n",
    "Si dos filas son similares en otras variables, se asume que el valor faltante tambi√©n ser√° similar.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "```\n",
    "\n",
    "*  **Ventajas:**\n",
    "\n",
    "  * Tiene en cuenta **correlaciones impl√≠citas** entre columnas.\n",
    "  * Es **no param√©trico** (no asume distribuci√≥n).\n",
    "*  **Desventajas:**\n",
    "\n",
    "  * Costoso en datasets grandes.\n",
    "  * Puede ser inestable si hay ruido o si los vecinos no son representativos.\n",
    "\n",
    "> üí° Si tus variables tienen una correlaci√≥n significativa, este m√©todo suele funcionar mejor que `SimpleImputer`.\n",
    "\n",
    "---\n",
    "\n",
    "### C. `IterativeImputer` (basado en MICE)\n",
    "\n",
    "**Idea:** este es el m√©todo m√°s avanzado ‚Äî usa un **modelo iterativo** para predecir los valores faltantes de cada columna **en funci√≥n de las dem√°s**.\n",
    "\n",
    "El algoritmo m√°s com√∫n es **MICE (Multiple Imputation by Chained Equations)**:\n",
    "\n",
    "1. Inicializa los `NaN` (por ejemplo, con medias).\n",
    "2. Toma una columna con valores faltantes y la trata como ‚Äúvariable objetivo‚Äù.\n",
    "3. Usa las otras columnas como features para entrenar un modelo (por ejemplo, regresi√≥n lineal o `BayesianRidge`).\n",
    "4. Predice los valores faltantes y los reemplaza.\n",
    "5. Repite para cada columna con nulos, y hace m√∫ltiples iteraciones hasta converger.\n",
    "\n",
    "```python\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "imputer = IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "```\n",
    "\n",
    "*  **Ventajas:**\n",
    "\n",
    "  * Captura la **correlaci√≥n entre variables**.\n",
    "  * Proporciona imputaciones m√°s **coherentes y realistas**.\n",
    "  * Compatible con modelos diferentes al lineal (puedes usar √°rboles, etc.).\n",
    "*  **Desventajas:**\n",
    "\n",
    "  * M√°s lento.\n",
    "  * Supone que las relaciones entre columnas son **predecibles y estables**.\n",
    "\n",
    "> üí° Si tus columnas tienen correlaci√≥n significativa (edad ‚Üî ingreso, educaci√≥n ‚Üî gasto, etc.), **IterativeImputer** es el m√°s adecuado.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Imputaci√≥n m√∫ltiple (`Multiple Imputation`)\n",
    "\n",
    "**IterativeImputer ‚â† Multiple Imputation**, aunque MICE naci√≥ de ese concepto.\n",
    "\n",
    "### üîç Idea:\n",
    "\n",
    "La imputaci√≥n m√∫ltiple reconoce que **hay incertidumbre** en los valores imputados.\n",
    "En lugar de generar una sola versi√≥n del dataset imputado, genera **m√∫ltiples datasets** con distintas imputaciones plausibles.\n",
    "\n",
    "1. Se crean **m** datasets con imputaciones diferentes (por ejemplo, 5 versiones).\n",
    "2. Se entrena el modelo en cada dataset.\n",
    "3. Se combinan los resultados (por ejemplo, promediando los coeficientes o predicciones).\n",
    "\n",
    "Esto permite:\n",
    "\n",
    "* Capturar la **variabilidad debida a la imputaci√≥n**.\n",
    "* Evitar sobreconfianza en los valores estimados.\n",
    "\n",
    "En Python puedes hacerlo con **`statsmodels`**, **`miceforest`** o **`autoimpute`**.\n",
    "\n",
    "Ejemplo usando [`miceforest`](https://pypi.org/project/miceforest/):\n",
    "\n",
    "```python\n",
    "import miceforest as mf\n",
    "import pandas as pd\n",
    "\n",
    "# Creamos el kernel (modelo base para imputaci√≥n m√∫ltiple)\n",
    "kernel = mf.ImputationKernel(df, save_all_iterations=True, random_state=42)\n",
    "\n",
    "# Ejecutamos el proceso MICE (crea m√∫ltiples datasets)\n",
    "kernel.mice(5)\n",
    "\n",
    "# Recuperamos las versiones imputadas\n",
    "imputed_datasets = [kernel.complete_data(i) for i in range(5)]\n",
    "```\n",
    "\n",
    "* ‚úÖ **Ventajas:**\n",
    "\n",
    "  * Captura la **incertidumbre** en la imputaci√≥n.\n",
    "  * Mejores inferencias estad√≠sticas.\n",
    "* **Desventajas:**\n",
    "\n",
    "  * M√°s complejo de implementar y combinar.\n",
    "  * No siempre necesario para tareas puramente predictivas.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ¬øCu√°ndo usar cada uno?\n",
    "\n",
    "| M√©todo                         | Usa correlaci√≥n | Nivel de complejidad | Mejor para                                       |\n",
    "| :----------------------------- | :-------------: | :------------------: | :----------------------------------------------- |\n",
    "| **SimpleImputer**              |        ‚ùå        |         Bajo         | Variables independientes o sin mucha correlaci√≥n |\n",
    "| **KNNImputer**                 |  ‚úÖ (impl√≠cita)  |         Medio        | Datasets medianos, correlaci√≥n no lineal         |\n",
    "| **IterativeImputer**           |  ‚úÖ (expl√≠cita)  |         Alto         | Datasets correlacionados, predictivos            |\n",
    "| **Multiple Imputation (MICE)** |        ‚úÖ‚úÖ       |       Muy alto       | An√°lisis estad√≠stico con incertidumbre medida    |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Recomendaci√≥n pr√°ctica\n",
    "\n",
    "Si tu foco es **modelado predictivo (ML)**:\n",
    "\n",
    "* Empieza con `IterativeImputer` (con `BayesianRidge` o `RandomForestRegressor`).\n",
    "* Eval√∫a la mejora frente a `SimpleImputer`.\n",
    "\n",
    "Si tu foco es **an√°lisis estad√≠stico riguroso** (inferencia o papers):\n",
    "\n",
    "* Usa **Multiple Imputation (MICE)** con librer√≠as como `miceforest` o `autoimpute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63902347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputacion por medio de Simple Imputer\n",
    "columna_simple_imputer = ['BMI']\n",
    "columnas_knn_imputer = ['Glucose', 'BloodPressure', 'Outcome']\n",
    "columnas_iterative_imputer = ['Pregnancies', 'SkinThickness', 'Insulin', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b79a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar patrones de missingness primero\n",
    "missing_patterns = df.isnull().mean().sort_values(ascending=False)\n",
    "print(\"Porcentaje de valores faltantes por columna:\")\n",
    "print(missing_patterns[missing_patterns > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb91467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegir m√©todo basado en el an√°lisis\n",
    "if missing_patterns.max() < 0.05:  # Pocos missing\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "else:  # Patrones complejos\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "\n",
    "# Crear nuevo DataFrame con las mismas columnas\n",
    "imputed_data = imputer.fit_transform(df)\n",
    "df_imputed = pd.DataFrame(imputed_data, columns=df.columns, index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generar_distribuciones(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41245110",
   "metadata": {},
   "outputs": [],
   "source": [
    "generar_distribuciones(df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92829000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Cargar tu dataset (asumiendo que ya est√° cargado como df)\n",
    "# df = pd.read_csv('data.csv')  # Si necesitas cargarlo\n",
    "\n",
    "print(\"Dataset original - Valores faltantes por columna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Definimos cu√°ntas imputaciones queremos\n",
    "\n",
    "n_imputations = 10\n",
    "imputed_dfs = []\n",
    "\n",
    "print(f\"\\nGenerando {n_imputations} imputaciones m√∫ltiples...\")\n",
    "\n",
    "for i in range(n_imputations):\n",
    "\n",
    "    imputer = IterativeImputer(\n",
    "        random_state=i, \n",
    "        max_iter=10,\n",
    "        sample_posterior=True  # Muestrea de la distribuci√≥n posterior\n",
    "    )\n",
    "\n",
    "    df_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(df), \n",
    "        columns=df.columns,\n",
    "        index=df.index  # Mantener el √≠ndice original\n",
    "    )\n",
    "\n",
    "    imputed_dfs.append(df_imputed)\n",
    "    print(f\"Imputaci√≥n {i+1}/{n_imputations} completada\")\n",
    "\n",
    "# Promediamos las versiones imputadas\n",
    "df_mean = pd.concat(imputed_dfs).groupby(level=0).mean()\n",
    "df_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeaf907",
   "metadata": {},
   "outputs": [],
   "source": [
    "generar_distribuciones(df_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e570347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_imputation(original_df, imputed_df, columns_to_compare=None):\n",
    "    \n",
    "    if columns_to_compare is None:\n",
    "        columns_to_compare = original_df.columns\n",
    "    \n",
    "    print(\"COMPARACI√ìN DE ESTAD√çSTICAS:\")\n",
    "    \n",
    "    for col in columns_to_compare:\n",
    "        if col in original_df.select_dtypes(include=[np.number]).columns:\n",
    "            orig_mean = original_df[col].mean()\n",
    "            orig_std = original_df[col].std()\n",
    "            imp_mean = imputed_df[col].mean()\n",
    "            imp_std = imputed_df[col].std()\n",
    "            \n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Media: {orig_mean:.2f} ‚Üí {imp_mean:.2f} (Œî: {imp_mean-orig_mean:+.2f})\")\n",
    "            print(f\"  Std:   {orig_std:.2f} ‚Üí {imp_std:.2f} (Œî: {imp_std-orig_std:+.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91327c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_imputation(df, df_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2333c59a",
   "metadata": {},
   "source": [
    "# Ejemplo Simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe84d3",
   "metadata": {},
   "source": [
    "#  1. Tipos de mecanismos de datos faltantes\n",
    "\n",
    "| Tipo                                    | Significado                                                                                        | Ejemplo                                                    | Efecto                                                                                           |\n",
    "| --------------------------------------- | -------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------ |\n",
    "| **MCAR** (Missing Completely At Random) | Los valores faltan completamente al azar. No dependen ni de otras variables ni de su propio valor. | Faltan filas por un error de carga o desconexi√≥n.          | üîπ **P√©rdida de eficiencia**, pero sin **sesgo**.                                                |\n",
    "| **MAR** (Missing At Random)             | Los valores faltan en funci√≥n de **otras variables observadas**, no de la variable faltante.       | Faltan ‚ÄúJob performance‚Äù m√°s frecuentemente para IQ bajos. | ‚ö†Ô∏è **Introduce sesgo**, pero puede corregirse con t√©cnicas modernas (IterativeImputer, MICE).    |\n",
    "| **MNAR** (Missing Not At Random)        | Los valores faltan **por la propia variable faltante**.                                            | Si personas con mal desempe√±o no reportan su resultado.    | ‚ùå **Sesgo fuerte**, dif√≠cil de corregir (se usan *selection models* o *pattern mixture models*). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b99633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"IQ\": [78, 82, 86, 90, 94, 98, 102, 106, 110, 114],\n",
    "    \"JobPerformance\": [7.693428, np.nan, np.nan, np.nan, 8.631693, 9.231726, 13.458426, 12.434869, 10.561051, 13.185120]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "simple_imputer = SimpleImputer(strategy='mean')\n",
    "df_simple = df.copy()\n",
    "df_simple['JobPerformance'] = simple_imputer.fit_transform(df[['JobPerformance']])\n",
    "\n",
    "# Imputa con la **media global**, sin usar IQ.\n",
    "# No respeta correlaci√≥n IQ‚ÄìPerformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "df_knn = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Usa IQ y JobPerformance juntos para encontrar los vecinos m√°s cercanos y estimar el valor faltante.\n",
    "# Preserva la correlaci√≥n de manera **no lineal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "df_knn = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af513b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "iter_imputer = IterativeImputer(estimator=BayesianRidge(), random_state=42)\n",
    "df_iter = pd.DataFrame(iter_imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Predice los valores faltantes de JobPerformance en funci√≥n de IQ mediante regresi√≥n bayesiana iterativa.\n",
    "# Excelente para **datos MAR** con correlaci√≥n fuerte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # habilita el imputer experimental\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Definimos cu√°ntas imputaciones queremos\n",
    "n_imputations = 10\n",
    "imputed_dfs = []\n",
    "\n",
    "# Repetimos el proceso con distintas semillas\n",
    "\n",
    "for i in range(n_imputations):\n",
    "    imputer = IterativeImputer(random_state=i, max_iter=10)\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "    imputed_dfs.append(df_imputed)\n",
    "\n",
    "# Promediamos las versiones imputadas (solo num√©ricas)\n",
    "df_mean = pd.concat(imputed_dfs).groupby(level=0).mean()\n",
    "\n",
    "df_mean\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "86400-data-science-i-diplomatura",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
