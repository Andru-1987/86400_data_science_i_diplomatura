{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20893c23",
   "metadata": {},
   "source": [
    "# (Extra) Imputacion de data y analisis de Valores Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f220dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cc7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_DATASET:str = r\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2936bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_columns:list[str] = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "columns_clean:list[str] = map( lambda x: x.lower(), _columns)\n",
    "\n",
    "df = pd.read_csv(URL_DATASET, header=None, sep=\",\",names=list(columns_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(0,np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.isna().mean() * 100).round(2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5270c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db0c60",
   "metadata": {},
   "source": [
    "## Analisiside cantidad de nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921f922",
   "metadata": {},
   "source": [
    "## Imputacion por la mediana\n",
    "- Mediana vs Media(Promedio)\n",
    "\n",
    "Para un proyecto de machine learning, la elección entre media, moda y mediana depende del tipo de datos y del contexto específico. Te explico cuándo usar cada una:\n",
    "\n",
    "## **Media (Promedio)**\n",
    "**Cuándo usarla:**\n",
    "- Con datos numéricos continuos sin valores atípicos extremos\n",
    "- Cuando la distribución es aproximadamente normal\n",
    "- Para variables como edad, ingresos, temperatura, etc. (sin outliers)\n",
    "\n",
    "**Ejemplo:** Si tienes edades como [25, 28, 30, 32, 35], la media (30) representa bien el conjunto.\n",
    "\n",
    "## **Mediana**\n",
    "**Cuándo usarla:**\n",
    "- Con datos numéricos que tienen valores atípicos (outliers)\n",
    "- Cuando la distribución es sesgada\n",
    "- Para datos ordinales\n",
    "- Es más robusta ante valores extremos\n",
    "\n",
    "**Ejemplo:** En ingresos como [30K, 35K, 40K, 45K, 500K], la mediana (40K) es más representativa que la media (130K).\n",
    "\n",
    "## **Moda**\n",
    "**Cuándo usarla:**\n",
    "- Con datos categóricos (texto, etiquetas)\n",
    "- Con datos numéricos discretos donde buscas el valor más frecuente\n",
    "- Para variables como género, país, categoría de producto, etc.\n",
    "\n",
    "**Ejemplo:** En una columna de países [México, Argentina, México, Brasil, México], la moda es \"México\".\n",
    "\n",
    "## **Recomendaciones prácticas:**\n",
    "\n",
    "1. **Analiza primero la distribución** de tus datos con histogramas o boxplots\n",
    "2. **Detecta outliers** antes de decidir\n",
    "3. **Considera el contexto del negocio** - ¿qué medida tiene más sentido para tu problema?\n",
    "4. **Prueba diferentes enfoques** y evalúa el impacto en tu modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generar_distribuciones(df: pd.DataFrame) -> None:\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    df.hist(bins=20, figsize=(16, 9), layout=(3, 3), color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    plt.suptitle('Distribuciones ', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa655e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "medianas = df.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bada4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputada_mediana = df.fillna(medianas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32459ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generar_distribuciones(df_imputada_mediana)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95dcb3d",
   "metadata": {},
   "source": [
    "## Usando tecnicas mas eficientes dado a la cantidad de datos nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87dbafc",
   "metadata": {},
   "source": [
    "## Missing Data Imputation\n",
    "\n",
    "Cuando tienes valores nulos (`NaN`) en un dataset, la **imputación** busca reemplazarlos con estimaciones razonables para que los modelos de machine learning o análisis estadísticos no se rompan ni sesguen.\n",
    "\n",
    "El **objetivo** es hacerlo de una forma que mantenga las **relaciones (correlaciones)** entre variables y **minimice el sesgo**.\n",
    "\n",
    "---\n",
    "\n",
    "## Métodos comunes de imputación en Scikit-Learn\n",
    "\n",
    "### A. `SimpleImputer`\n",
    "\n",
    "**Idea:** reemplaza los valores faltantes con una **constante**, **media**, **mediana** o **moda** de la columna.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"edad\": [25, np.nan, 30, 40],\n",
    "    \"ingreso\": [50000, 60000, np.nan, 80000]\n",
    "})\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "```\n",
    "\n",
    "* **Ventajas:** simple, rápido, útil cuando los datos son poco correlacionados.\n",
    "* **Desventajas:** ignora la relación entre columnas; cada columna se imputa de forma independiente.\n",
    "\n",
    ">  Si tus variables están correlacionadas (por ejemplo, edad e ingreso), este método puede introducir sesgos.\n",
    "\n",
    "---\n",
    "\n",
    "### B. `KNNImputer`\n",
    "\n",
    "**Idea:** usa los **vecinos más cercanos** (por distancia euclídea entre filas completas) para imputar los valores faltantes.\n",
    "Si dos filas son similares en otras variables, se asume que el valor faltante también será similar.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "```\n",
    "\n",
    "*  **Ventajas:**\n",
    "\n",
    "  * Tiene en cuenta **correlaciones implícitas** entre columnas.\n",
    "  * Es **no paramétrico** (no asume distribución).\n",
    "*  **Desventajas:**\n",
    "\n",
    "  * Costoso en datasets grandes.\n",
    "  * Puede ser inestable si hay ruido o si los vecinos no son representativos.\n",
    "\n",
    "> 💡 Si tus variables tienen una correlación significativa, este método suele funcionar mejor que `SimpleImputer`.\n",
    "\n",
    "---\n",
    "\n",
    "### C. `IterativeImputer` (basado en MICE)\n",
    "\n",
    "**Idea:** este es el método más avanzado — usa un **modelo iterativo** para predecir los valores faltantes de cada columna **en función de las demás**.\n",
    "\n",
    "El algoritmo más común es **MICE (Multiple Imputation by Chained Equations)**:\n",
    "\n",
    "1. Inicializa los `NaN` (por ejemplo, con medias).\n",
    "2. Toma una columna con valores faltantes y la trata como “variable objetivo”.\n",
    "3. Usa las otras columnas como features para entrenar un modelo (por ejemplo, regresión lineal o `BayesianRidge`).\n",
    "4. Predice los valores faltantes y los reemplaza.\n",
    "5. Repite para cada columna con nulos, y hace múltiples iteraciones hasta converger.\n",
    "\n",
    "```python\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "imputer = IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "```\n",
    "\n",
    "*  **Ventajas:**\n",
    "\n",
    "  * Captura la **correlación entre variables**.\n",
    "  * Proporciona imputaciones más **coherentes y realistas**.\n",
    "  * Compatible con modelos diferentes al lineal (puedes usar árboles, etc.).\n",
    "*  **Desventajas:**\n",
    "\n",
    "  * Más lento.\n",
    "  * Supone que las relaciones entre columnas son **predecibles y estables**.\n",
    "\n",
    "> 💡 Si tus columnas tienen correlación significativa (edad ↔ ingreso, educación ↔ gasto, etc.), **IterativeImputer** es el más adecuado.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Imputación múltiple (`Multiple Imputation`)\n",
    "\n",
    "**IterativeImputer ≠ Multiple Imputation**, aunque MICE nació de ese concepto.\n",
    "\n",
    "### 🔍 Idea:\n",
    "\n",
    "La imputación múltiple reconoce que **hay incertidumbre** en los valores imputados.\n",
    "En lugar de generar una sola versión del dataset imputado, genera **múltiples datasets** con distintas imputaciones plausibles.\n",
    "\n",
    "1. Se crean **m** datasets con imputaciones diferentes (por ejemplo, 5 versiones).\n",
    "2. Se entrena el modelo en cada dataset.\n",
    "3. Se combinan los resultados (por ejemplo, promediando los coeficientes o predicciones).\n",
    "\n",
    "Esto permite:\n",
    "\n",
    "* Capturar la **variabilidad debida a la imputación**.\n",
    "* Evitar sobreconfianza en los valores estimados.\n",
    "\n",
    "En Python puedes hacerlo con **`statsmodels`**, **`miceforest`** o **`autoimpute`**.\n",
    "\n",
    "Ejemplo usando [`miceforest`](https://pypi.org/project/miceforest/):\n",
    "\n",
    "```python\n",
    "import miceforest as mf\n",
    "import pandas as pd\n",
    "\n",
    "# Creamos el kernel (modelo base para imputación múltiple)\n",
    "kernel = mf.ImputationKernel(df, save_all_iterations=True, random_state=42)\n",
    "\n",
    "# Ejecutamos el proceso MICE (crea múltiples datasets)\n",
    "kernel.mice(5)\n",
    "\n",
    "# Recuperamos las versiones imputadas\n",
    "imputed_datasets = [kernel.complete_data(i) for i in range(5)]\n",
    "```\n",
    "\n",
    "* ✅ **Ventajas:**\n",
    "\n",
    "  * Captura la **incertidumbre** en la imputación.\n",
    "  * Mejores inferencias estadísticas.\n",
    "* **Desventajas:**\n",
    "\n",
    "  * Más complejo de implementar y combinar.\n",
    "  * No siempre necesario para tareas puramente predictivas.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ¿Cuándo usar cada uno?\n",
    "\n",
    "| Método                         | Usa correlación | Nivel de complejidad | Mejor para                                       |\n",
    "| :----------------------------- | :-------------: | :------------------: | :----------------------------------------------- |\n",
    "| **SimpleImputer**              |        ❌        |         Bajo         | Variables independientes o sin mucha correlación |\n",
    "| **KNNImputer**                 |  ✅ (implícita)  |         Medio        | Datasets medianos, correlación no lineal         |\n",
    "| **IterativeImputer**           |  ✅ (explícita)  |         Alto         | Datasets correlacionados, predictivos            |\n",
    "| **Multiple Imputation (MICE)** |        ✅✅       |       Muy alto       | Análisis estadístico con incertidumbre medida    |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Recomendación práctica\n",
    "\n",
    "Si tu foco es **modelado predictivo (ML)**:\n",
    "\n",
    "* Empieza con `IterativeImputer` (con `BayesianRidge` o `RandomForestRegressor`).\n",
    "* Evalúa la mejora frente a `SimpleImputer`.\n",
    "\n",
    "Si tu foco es **análisis estadístico riguroso** (inferencia o papers):\n",
    "\n",
    "* Usa **Multiple Imputation (MICE)** con librerías como `miceforest` o `autoimpute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63902347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputacion por medio de Simple Imputer\n",
    "columna_simple_imputer = ['BMI']\n",
    "columnas_knn_imputer = ['Glucose', 'BloodPressure', 'Outcome']\n",
    "columnas_iterative_imputer = ['Pregnancies', 'SkinThickness', 'Insulin', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b79a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar patrones de missingness primero\n",
    "missing_patterns = df.isnull().mean().sort_values(ascending=False)\n",
    "print(\"Porcentaje de valores faltantes por columna:\")\n",
    "print(missing_patterns[missing_patterns > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb91467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegir método basado en el análisis\n",
    "if missing_patterns.max() < 0.05:  # Pocos missing\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "else:  # Patrones complejos\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "\n",
    "# Crear nuevo DataFrame con las mismas columnas\n",
    "imputed_data = imputer.fit_transform(df)\n",
    "df_imputed = pd.DataFrame(imputed_data, columns=df.columns, index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generar_distribuciones(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41245110",
   "metadata": {},
   "outputs": [],
   "source": [
    "generar_distribuciones(df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92829000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Cargar tu dataset (asumiendo que ya está cargado como df)\n",
    "# df = pd.read_csv('data.csv')  # Si necesitas cargarlo\n",
    "\n",
    "print(\"Dataset original - Valores faltantes por columna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Definimos cuántas imputaciones queremos\n",
    "\n",
    "n_imputations = 10\n",
    "imputed_dfs = []\n",
    "\n",
    "print(f\"\\nGenerando {n_imputations} imputaciones múltiples...\")\n",
    "\n",
    "for i in range(n_imputations):\n",
    "\n",
    "    imputer = IterativeImputer(\n",
    "        random_state=i, \n",
    "        max_iter=10,\n",
    "        sample_posterior=True  # Muestrea de la distribución posterior\n",
    "    )\n",
    "\n",
    "    df_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(df), \n",
    "        columns=df.columns,\n",
    "        index=df.index  # Mantener el índice original\n",
    "    )\n",
    "\n",
    "    imputed_dfs.append(df_imputed)\n",
    "    print(f\"Imputación {i+1}/{n_imputations} completada\")\n",
    "\n",
    "# Promediamos las versiones imputadas\n",
    "df_mean = pd.concat(imputed_dfs).groupby(level=0).mean()\n",
    "df_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeaf907",
   "metadata": {},
   "outputs": [],
   "source": [
    "generar_distribuciones(df_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e570347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_imputation(original_df, imputed_df, columns_to_compare=None):\n",
    "    \n",
    "    if columns_to_compare is None:\n",
    "        columns_to_compare = original_df.columns\n",
    "    \n",
    "    print(\"COMPARACIÓN DE ESTADÍSTICAS:\")\n",
    "    \n",
    "    for col in columns_to_compare:\n",
    "        if col in original_df.select_dtypes(include=[np.number]).columns:\n",
    "            orig_mean = original_df[col].mean()\n",
    "            orig_std = original_df[col].std()\n",
    "            imp_mean = imputed_df[col].mean()\n",
    "            imp_std = imputed_df[col].std()\n",
    "            \n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Media: {orig_mean:.2f} → {imp_mean:.2f} (Δ: {imp_mean-orig_mean:+.2f})\")\n",
    "            print(f\"  Std:   {orig_std:.2f} → {imp_std:.2f} (Δ: {imp_std-orig_std:+.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91327c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_imputation(df, df_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2333c59a",
   "metadata": {},
   "source": [
    "# Ejemplo Simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe84d3",
   "metadata": {},
   "source": [
    "#  1. Tipos de mecanismos de datos faltantes\n",
    "\n",
    "| Tipo                                    | Significado                                                                                        | Ejemplo                                                    | Efecto                                                                                           |\n",
    "| --------------------------------------- | -------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------ |\n",
    "| **MCAR** (Missing Completely At Random) | Los valores faltan completamente al azar. No dependen ni de otras variables ni de su propio valor. | Faltan filas por un error de carga o desconexión.          | 🔹 **Pérdida de eficiencia**, pero sin **sesgo**.                                                |\n",
    "| **MAR** (Missing At Random)             | Los valores faltan en función de **otras variables observadas**, no de la variable faltante.       | Faltan “Job performance” más frecuentemente para IQ bajos. | ⚠️ **Introduce sesgo**, pero puede corregirse con técnicas modernas (IterativeImputer, MICE).    |\n",
    "| **MNAR** (Missing Not At Random)        | Los valores faltan **por la propia variable faltante**.                                            | Si personas con mal desempeño no reportan su resultado.    | ❌ **Sesgo fuerte**, difícil de corregir (se usan *selection models* o *pattern mixture models*). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b99633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"IQ\": [78, 82, 86, 90, 94, 98, 102, 106, 110, 114],\n",
    "    \"JobPerformance\": [7.693428, np.nan, np.nan, np.nan, 8.631693, 9.231726, 13.458426, 12.434869, 10.561051, 13.185120]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "simple_imputer = SimpleImputer(strategy='mean')\n",
    "df_simple = df.copy()\n",
    "df_simple['JobPerformance'] = simple_imputer.fit_transform(df[['JobPerformance']])\n",
    "\n",
    "# Imputa con la **media global**, sin usar IQ.\n",
    "# No respeta correlación IQ–Performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "df_knn = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Usa IQ y JobPerformance juntos para encontrar los vecinos más cercanos y estimar el valor faltante.\n",
    "# Preserva la correlación de manera **no lineal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "df_knn = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af513b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "iter_imputer = IterativeImputer(estimator=BayesianRidge(), random_state=42)\n",
    "df_iter = pd.DataFrame(iter_imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Predice los valores faltantes de JobPerformance en función de IQ mediante regresión bayesiana iterativa.\n",
    "# Excelente para **datos MAR** con correlación fuerte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # habilita el imputer experimental\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Definimos cuántas imputaciones queremos\n",
    "n_imputations = 10\n",
    "imputed_dfs = []\n",
    "\n",
    "# Repetimos el proceso con distintas semillas\n",
    "\n",
    "for i in range(n_imputations):\n",
    "    imputer = IterativeImputer(random_state=i, max_iter=10)\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "    imputed_dfs.append(df_imputed)\n",
    "\n",
    "# Promediamos las versiones imputadas (solo numéricas)\n",
    "df_mean = pd.concat(imputed_dfs).groupby(level=0).mean()\n",
    "\n",
    "df_mean\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "86400-data-science-i-diplomatura",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
