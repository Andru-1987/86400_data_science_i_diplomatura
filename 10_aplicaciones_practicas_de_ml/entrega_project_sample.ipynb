{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a59c14",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "\n",
    "## Modelado Predictivo de la Satisfacción de Pasajeros en Aerolíneas Low-Cost: Un Enfoque de Machine Learning para Reseñas de Clientes de Ryanair\n",
    "\n",
    "**Contexto:** La industria de aerolíneas de bajo costo opera en un equilibrio delicado entre precios competitivos y calidad de servicio, haciendo de la satisfacción del pasajero un determinante crítico del éxito empresarial. Ryanair, como la mayor aerolínea low-cost de Europa, presenta un caso de estudio compelling debido a su modelo de negocio distintivo y percepciones polarizadas de los clientes.\n",
    "\n",
    "**Objetivo de Investigación:** Este estudio desarrolla un framework predictivo integral para analizar los determinantes del comportamiento de recomendación de pasajeros, integrando métricas estructuradas de rating con feedback textual no estructurado. La investigación aborda una brecha significativa en la literatura de aviación mediante el empleo de técnicas avanzadas de machine learning para decodificar la compleja interacción entre atributos de servicio, demographics de pasajeros y patrones lingüísticos en la formación de satisfacción.\n",
    "\n",
    "**Metodología:** Utilizando un dataset de 276 reseñas detalladas de pasajeros, implementamos un enfoque analítico multimodal que combina:\n",
    "- Análisis tradicional de ratings (Comodidad del Asiento, Servicio de Tripulación, Valor por Dinero, etc.)\n",
    "- Rendimiento comparativo de tres algoritmos de clasificación: Regresión Logística, Random Forest y XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee3b850",
   "metadata": {},
   "source": [
    "## Hipótesis Extendidas para Investigación\n",
    "\n",
    "**Hipótesis 1: \"Hipótesis de Primacía del Valor Económico\"**\n",
    "La percepción de valor por dinero supera a todos los demás atributos de servicio en la determinación de la satisfacción general y la probabilidad de recomendación en el contexto de aerolíneas low-cost.\n",
    "\n",
    "**Hipótesis 2:  \"Hipótesis Temporal\"**\n",
    "**\"La satisfacción ha mejorado/deteriorado consistentemente en el tiempo\"**\n",
    "- *Variable*: `date_flown` o `date_published`\n",
    "- Análisis de tendencias temporales\n",
    "\n",
    "**Hipótesis 3: \"Hipótesis de Vulnerabilidad del Segmento de Pasajeros\"**\n",
    "Ciertos segmentos de pasajeros (particularmente familias y parejas de leisure) demuestran sistemáticamente menor satisfacción debido expectativas desalineadas del servicio y sensibilidad a cargos adicionales.\n",
    "\n",
    "**Conclusion de trabajo a realizar**\n",
    "Este estudio desarrolla un modelo de clasificación para predecir la satisfacción de pasajeros con Ryanair basado en reseñas online, combinando análisis de texto (comentarios) con métricas estructuradas de evaluación. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1862d40",
   "metadata": {},
   "source": [
    "**Estructura del dataset:**\n",
    "- **date_published**: Fecha de publicación de la reseña\n",
    "- **overall_rating**: Calificación general (1-10)\n",
    "- **passenger_country**: País del pasajero\n",
    "- **trip_verified**: Si el viaje fue verificado\n",
    "- **aircraft**: Tipo de avión (principalmente Boeing 737 variantes)\n",
    "- **type_of_traveller**: Tipo de viajero (Leisure, Business, etc.)\n",
    "- **seat_type**: Clase del asiento\n",
    "- **origin/destination**: Aeropuertos de origen y destino\n",
    "- **date_flown**: Fecha del vuelo\n",
    "- **Métricas de evaluación**: seat_comfort, cabin_staff_service, food_&_beverages, ground_service, value_for_money, inflight_entertainment, wifi_&_connectivity\n",
    "- **recommended**: Si recomendaría la aerolínea (yes/no)\n",
    "\n",
    "**Observaciones iniciales:**\n",
    "- Hay aproximadamente 900 reseñas\n",
    "- Las calificaciones van desde 1.0 (muy malas) hasta 10.0 (excelentes)\n",
    "- La mayoría de los vuelos son en clase económica\n",
    "- Los pasajeros provienen de diversos países\n",
    "- Los destinos cubren principalmente Europa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff21b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd50ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "file_path = \"./dataset/ryanair_reviews.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36768a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mostrar información general del dataset\n",
    "print(\"Shape\")\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Columns\")\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763bded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ec801",
   "metadata": {},
   "source": [
    "_En base de algunos tipos de datos, eliminaremos las columnas que no tienen sentido para la recomendacion, por ejemplo el no uso de comentaros o data del tipo NLP_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c85bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    \"id_review\",\n",
    "    \"comment_title\",\n",
    "    \"comment\"\n",
    "]\n",
    "\n",
    "target = ['recommended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\").str.replace(\"(\", \"\").str.replace(\")\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(df):\n",
    "\n",
    "    # Calcular valores nulos\n",
    "    missing_values = df.isna().sum()\n",
    "    missing_percentage = ((df.isna().sum() / len(df)) * 100).round(2)\n",
    "\n",
    "    # Crear DataFrame\n",
    "    missing_df = pd.DataFrame({\n",
    "        \"Cantidad_Nulos\": missing_values,\n",
    "        \"Porcentaje_Nulos\": missing_percentage\n",
    "    })\n",
    "\n",
    "    # Ordenar por porcentaje de nulos\n",
    "    missing_df = missing_df.sort_values(by=\"Porcentaje_Nulos\", ascending=False)    \n",
    "    return missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f5076",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = analyze_missing_values(df)\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa6c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_high_missing_amount = missing_df.loc[missing_df.Porcentaje_Nulos>=40].index\n",
    "columns_medium_missing_amount = missing_df.loc[(missing_df.Porcentaje_Nulos<40) & (missing_df.Porcentaje_Nulos>4)].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d10550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"value_for_money\"].value_counts(dropna=False,normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71300c55",
   "metadata": {},
   "source": [
    "_Para este caso muy puntual tiene la posibilidad de drop rows --> por la cantidad de datos presentados_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd230b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=\"value_for_money\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc90996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_drop = [\"cabin_staff_service\",\"seat_comfort\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820025ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=value_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c29cca",
   "metadata": {},
   "source": [
    "_Eliminacion de duplicados_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_with_info(df, subset=None, keep='first'):\n",
    "    print(\"INFORMACIÓN DE ENTRADA:\")\n",
    "    print(f\"   - Dimensiones: {df.shape[0]} filas × {df.shape[1]} columnas\")\n",
    "    print(f\"   - Duplicados totales: {df.duplicated(subset=subset).sum()}\")\n",
    "    \n",
    "    if subset:\n",
    "        print(f\"   - Verificando duplicados en columnas: {subset}\")\n",
    "    \n",
    "    original_rows = df.shape[0]\n",
    "    \n",
    "    df_clean = df.drop_duplicates(subset=subset, keep=keep)\n",
    "    \n",
    "    print(\"\\n INFORMACIÓN DE SALIDA:\")\n",
    "    print(f\"   - Dimensiones: {df_clean.shape[0]} filas × {df_clean.shape[1]} columnas\")\n",
    "    print(f\"   - Duplicados restantes: {df_clean.duplicated(subset=subset).sum()}\")\n",
    "    print(f\"   - Filas eliminadas: {original_rows - df_clean.shape[0]}\")\n",
    "    print(f\"   - Reducción: {((original_rows - df_clean.shape[0]) / original_rows * 100):.1f}%\")\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58054de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_duplicates_with_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd26c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_unique_count(df, columns=None):\n",
    "\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    \n",
    "    print(\"CONTEO DE VALORES ÚNICOS POR COLUMNA\")\n",
    "\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "\n",
    "            unique_count = df[col].nunique()\n",
    "            total_count = len(df[col])\n",
    "            null_count = df[col].isnull().sum()\n",
    "            moda_value = df[col].mode()\n",
    "\n",
    "\n",
    "            print(f\" {col}:\")\n",
    "            print(f\"   • Tipo: {df[col].dtype}\")\n",
    "            print(f\"   • Valores únicos: {unique_count:,}\")\n",
    "            print(f\"   • Valor de la moda: {moda_value}\")\n",
    "            print(f\"   • Total valores: {total_count:,}\")\n",
    "            print(f\"   • Valores nulos: {null_count:,}\")\n",
    "            print(f\"   % Únicos: {(unique_count/total_count*100):.1f}%\")\n",
    "            \n",
    "            # Mostrar algunos valores únicos (especial manejo para floats)\n",
    "            if unique_count <= 10:\n",
    "                unique_vals = df[col].dropna().unique()\n",
    "                print(f\"   • Valores: {sorted(unique_vals)}\")\n",
    "            elif df[col].dtype in ['float64', 'float32']:\n",
    "                # Para floats, mostrar rango\n",
    "                min_val = df[col].min()\n",
    "                max_val = df[col].max()\n",
    "                print(f\"   • Rango: [{min_val:.2f} - {max_val:.2f}]\")\n",
    "            else:\n",
    "                sample_vals = df[col].dropna().unique()[:3]\n",
    "                print(f\"   • Ejemplos: {list(sample_vals)}\")\n",
    "            \n",
    "            print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import pad\n",
    "\n",
    "\n",
    "def plot_by_dtype_subplots(df, columns=None):\n",
    "    if columns is None:\n",
    "        columns = df.columns.tolist()\n",
    "    \n",
    "    # Separar por tipo de dato\n",
    "    numeric_cols = df[columns].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df[columns].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"Columnas numéricas: {len(numeric_cols)}\")\n",
    "    print(f\"Columnas categóricas: {len(categorical_cols)}\")\n",
    "    \n",
    "    if numeric_cols:\n",
    "        n_numeric = len(numeric_cols)\n",
    "        n_rows = (n_numeric + 2) // 3\n",
    "        fig, axes = plt.subplots(n_rows, 3, figsize=(18, n_rows * 4))\n",
    "        \n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        fig.suptitle('Distribución de Columnas Numéricas')\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols):\n",
    "            row = i // 3\n",
    "            col_ax = i % 3\n",
    "            ax = axes[row, col_ax]\n",
    "            \n",
    "            df[col].hist(bins=30, ax=ax, color='lightblue', alpha=0.7, edgecolor='black')\n",
    "            ax.set_title(f'{col}\\n(Únicos: {df[col].nunique()})', fontweight='bold')\n",
    "            ax.set_ylabel('Frecuencia')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Ocultar ejes vacíos\n",
    "        for i in range(len(numeric_cols), n_rows * 3):\n",
    "            row = i // 3\n",
    "            col_ax = i % 3\n",
    "            axes[row, col_ax].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Graficar categóricas\n",
    "    if categorical_cols:\n",
    "        n_categorical = len(categorical_cols)\n",
    "        n_rows = (n_categorical + 2) // 3\n",
    "        fig, axes = plt.subplots(n_rows, 3, figsize=(18, n_rows * 4))\n",
    "        \n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -3)\n",
    "        \n",
    "        fig.suptitle('Distribución de Columnas Categóricas')\n",
    "        \n",
    "        for i, col in enumerate(categorical_cols):\n",
    "            row = i // 3\n",
    "            col_ax = i % 3\n",
    "            ax = axes[row, col_ax]\n",
    "            \n",
    "            value_counts = df[col].value_counts().head(10)  # Top 10\n",
    "            bars = ax.bar(range(len(value_counts)), value_counts.values, \n",
    "                         color=plt.cm.Pastel1(i / len(categorical_cols)), alpha=0.7)\n",
    "            \n",
    "            ax.set_xticks(range(len(value_counts)))\n",
    "            ax.set_xticklabels([str(x)[:10] + '...' if len(str(x)) > 10 else str(x) \n",
    "                              for x in value_counts.index], \n",
    "                             rotation=45, ha='right', fontsize=8)\n",
    "            \n",
    "            # Añadir valores en barras\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{int(height)}', ha='center', va='bottom', fontsize=7)\n",
    "            \n",
    "            ax.set_title(f'{col}\\n(Únicos: {df[col].nunique()})', fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Ocultar ejes vacíos\n",
    "        for i in range(len(categorical_cols), n_rows * 3):\n",
    "            row = i // 3\n",
    "            col_ax = i % 3\n",
    "            axes[row, col_ax].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98633e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_unique_count(df[columns_high_missing_amount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_dtype_subplots(df, columns_high_missing_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ccc6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_unique_count(df[columns_medium_missing_amount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_dtype_subplots(df, columns_medium_missing_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b597d",
   "metadata": {},
   "source": [
    "### Imputacion por uso de simple imputerss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab5d06",
   "metadata": {},
   "source": [
    "_Si bien tienen un alto valor de nulls, tiene sentido plantear que son valores nulls por default, por lo tanto vamos a utilizar imputacion por medio de valores de moda y otros por valores `0`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05395630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46effcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para el caso de food_&_beverages\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "imputed_values = imputer.fit_transform(df[['food_&_beverages']])\n",
    "\n",
    "df['food_&_beverages'] = np.round(imputed_values).clip(0, 5)\n",
    "\n",
    "print(\"Valores únicos después de imputación:\", df['food_&_beverages'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_impute_numeric = [\"inflight_entertainment\",\"wifi_&_connectivity\"]\n",
    "columns_to_impute_categoric = [\"aircraft\",\"trip_verified\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb2d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputador para numéricas con valor constante 0\n",
    "numeric_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "df[columns_to_impute_numeric] = numeric_imputer.fit_transform(df[columns_to_impute_numeric])\n",
    "\n",
    "\n",
    "# Imputador para categóricas con la moda\n",
    "categoric_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[columns_to_impute_categoric] = categoric_imputer.fit_transform(df[columns_to_impute_categoric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af780c87",
   "metadata": {},
   "source": [
    "_date values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6acd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date_flown\"] = pd.to_datetime(df[\"date_flown\"], format = \"%B %Y\", errors=\"coerce\")                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f40517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear características temporales antes de imputar\n",
    "df['year'] = df['date_flown'].dt.year\n",
    "df['month'] = df['date_flown'].dt.month\n",
    "df['quarter'] = df['date_flown'].dt.quarter\n",
    "\n",
    "# Imputar la fecha principal con moda\n",
    "mode_date = df['date_flown'].mode()[0] if not df['date_flown'].mode().empty else pd.Timestamp('2019-09-01')\n",
    "df['date_flown'].fillna(mode_date, inplace=True)\n",
    "\n",
    "# Verificar resultado\n",
    "print(f\"Valores nulos restantes: {df['date_flown'].isnull().sum()}\")\n",
    "print(f\"Rango de fechas: {df['date_flown'].min()} to {df['date_flown'].max()}\")\n",
    "\n",
    "df.drop(columns=['year','month','quarter'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "constant_columns = [\"ground_service\" ,\"overall_rating\"] \n",
    "\n",
    "iterative_columns = [\"cabin_staff_service\", \"seat_comfort\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b517024",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "imputed_values = imputer.fit_transform(df[constant_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputador para numéricas con valor constante 0\n",
    "numeric_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "df[constant_columns] = numeric_imputer.fit_transform(df[constant_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"origin\",\"destination\",\"type_of_traveller\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar la combinación origen-destino más frecuente\n",
    "route_mode = df.groupby(['origin', 'destination']).size().idxmax()\n",
    "\n",
    "# Imputar pares de forma coordinada\n",
    "mask = df['origin'].isna() & df['destination'].isna()\n",
    "df.loc[mask, 'origin'] = route_mode[0]\n",
    "df.loc[mask, 'destination'] = route_mode[1]\n",
    "\n",
    "df['origin'] = df['origin'].fillna('Unknown')\n",
    "df['destination'] = df['destination'].fillna('Unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uso de la moda para la imputacion\n",
    "imputed_values = SimpleImputer(strategy='most_frequent').fit_transform(df[['type_of_traveller']])\n",
    "df['type_of_traveller'] = imputed_values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d63cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a399246",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94993353",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_dtype_subplots(df, df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b6163",
   "metadata": {},
   "source": [
    "## Hipotesis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e81d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_corr =[col.lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\") for col in\n",
    "['Overall Rating', 'Value For Money', 'Seat Comfort', 'Cabin Staff Service', 'Ground Service']]\n",
    "\n",
    "columns_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de correlación entre value_for_money y overall_rating\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Gráfico 1: Correlación entre value_for_money y overall_rating\n",
    "plt.subplot(1, 3, 1)\n",
    "correlation_matrix = df[columns_corr].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlación entre Ratings')\n",
    "\n",
    "# Gráfico 2: Distribución de overall_rating vs value_for_money\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(data=df, x='value_for_money', y='overall_rating', alpha=0.6)\n",
    "plt.title('value_for_money vs overall_rating')\n",
    "plt.xlabel('value_for_money Rating')\n",
    "plt.ylabel('overall_rating')\n",
    "\n",
    "# Gráfico 3: Comparación de importancia de factores\n",
    "plt.subplot(1, 3, 3)\n",
    "factors = columns_corr\n",
    "correlations_with_overall = [correlation_matrix.loc['overall_rating', factor] for factor in factors]\n",
    "\n",
    "plt.bar(factors, correlations_with_overall, color=['red', 'blue', 'green', 'orange'])\n",
    "plt.title('Correlación con overall_rating')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis estadístico\n",
    "print(\"ANÁLISIS HIPÓTESIS 1\")\n",
    "print(f\"Correlación value_for_money - overall_rating: {correlation_matrix.loc['overall_rating', 'value_for_money']:.3f}\")\n",
    "print(f\"Correlación Seat Comfort - overall_rating: {correlation_matrix.loc['overall_rating', 'seat_comfort']:.3f}\")\n",
    "print(f\"Correlación Cabin Staff - overall_rating: {correlation_matrix.loc['overall_rating', 'cabin_staff_service']:.3f}\")\n",
    "\n",
    "# Test de significancia\n",
    "from scipy.stats import pearsonr\n",
    "corr_val, p_val = pearsonr(df['value_for_money'].dropna(), \n",
    "                          df['overall_rating'].dropna())\n",
    "print(f\"Valor p para correlación value_for_money: {p_val:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65034b9c",
   "metadata": {},
   "source": [
    "*El análisis revela que \"Value For Money\" tiene la correlación más fuerte con el Overall Rating (correlación de ~0.85), significativamente mayor que otros factores como comodidad del asiento o servicio de cabina. Esto valida la hipótesis de que en aerolíneas low-cost, la percepción de valor económico es el principal driver de satisfacción.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, chi2_contingency, f_oneway\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"HIPÓTESIS 2: Evolución Temporal de la Satisfacción\")\n",
    "\n",
    "df['year_month'] = df['date_flown'].dt.to_period('M')\n",
    "\n",
    "# Preparar datos temporales\n",
    "temporal_data = df.dropna(subset=['date_flown', 'overall_rating']).copy()\n",
    "\n",
    "monthly_stats = temporal_data.groupby('year_month').agg({\n",
    "    'overall_rating': ['mean', 'count', 'std'],\n",
    "    'value_for_money': 'mean',\n",
    "    'cabin_staff_service': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "monthly_stats.columns = ['rating_mean', 'review_count', 'rating_std', \n",
    "                        'value_mean', 'staff_mean']\n",
    "monthly_stats = monthly_stats[monthly_stats['review_count'] >= 5]  # Filtrar meses con suficientes datos\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Gráfico 1: Evolución del rating promedio\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(monthly_stats.index.astype(str), monthly_stats['rating_mean'], \n",
    "         marker='o', linewidth=2, markersize=4, color='blue', alpha=0.7)\n",
    "\n",
    "plt.fill_between(monthly_stats.index.astype(str), \n",
    "                monthly_stats['rating_mean'] - monthly_stats['rating_std'],\n",
    "                monthly_stats['rating_mean'] + monthly_stats['rating_std'],\n",
    "                alpha=0.2, color='blue')\n",
    "plt.title('Evolución del Rating Promedio Mensual', fontweight='bold')\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.ylabel('Overall Rating Promedio')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Gráfico 2: Evolución comparada de factores\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(monthly_stats.index.astype(str), monthly_stats['rating_mean'], \n",
    "         marker='o', label='Overall Rating', linewidth=2)\n",
    "plt.plot(monthly_stats.index.astype(str), monthly_stats['value_mean'], \n",
    "         marker='s', label='Value for Money', linewidth=2)\n",
    "plt.plot(monthly_stats.index.astype(str), monthly_stats['staff_mean'], \n",
    "         marker='^', label='Cabin Staff', linewidth=2)\n",
    "plt.title('Evolución Comparada de Factores Clave', fontweight='bold')\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.ylabel('Rating Promedio')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.bar(monthly_stats.index.astype(str), monthly_stats['review_count'], \n",
    "        color='green', alpha=0.7)\n",
    "plt.title('Número de Reseñas por Mes', fontweight='bold')\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.ylabel('Cantidad de Reseñas')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "temporal_data['year'] = temporal_data['date_flown'].dt.year\n",
    "yearly_boxplot = temporal_data[temporal_data['year'] >= 2020]  # Filtrar años recientes\n",
    "sns.boxplot(data=yearly_boxplot, x='year', y='overall_rating')\n",
    "plt.title('Distribución de Ratings por Año', fontweight='bold')\n",
    "plt.xlabel('Año')\n",
    "plt.ylabel('Overall Rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis estadístico Hipótesis 2\n",
    "print(\"\\n ANÁLISIS ESTADÍSTICO HIPÓTESIS 2\")\n",
    "print(\"Tendencia temporal del rating promedio:\")\n",
    "print(f\"Rating promedio inicial: {monthly_stats['rating_mean'].iloc[0]:.2f}\")\n",
    "print(f\"Rating promedio final: {monthly_stats['rating_mean'].iloc[-1]:.2f}\")\n",
    "print(f\"Diferencia: {monthly_stats['rating_mean'].iloc[-1] - monthly_stats['rating_mean'].iloc[0]:.2f}\")\n",
    "\n",
    "# Test de correlación temporal\n",
    "monthly_stats_reset = monthly_stats.reset_index()\n",
    "monthly_stats_reset['time_index'] = range(len(monthly_stats_reset))\n",
    "time_corr, time_p = pearsonr(monthly_stats_reset['time_index'], \n",
    "                            monthly_stats_reset['rating_mean'])\n",
    "print(f\"Correlación temporal (rating vs tiempo): {time_corr:.3f}, p-value: {time_p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b769196",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2. HIPÓTESIS TEMPORAL:\")\n",
    "if abs(time_corr) > 0.3 and time_p < 0.05:\n",
    "    trend = \"mejorado\" if time_corr > 0 else \"deteriorado\"\n",
    "    print(f\"CONFIRMADA: Satisfacción ha {trend} significativamente en el tiempo\")\n",
    "else:\n",
    "    print(\"NO CONFIRMADA: No hay tendencia temporal significativa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50210bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HIPÓTESIS 3: Vulnerabilidad del Segmento de Pasajeros\")\n",
    "\n",
    "# Preparar datos por tipo de viajero\n",
    "traveler_data = df.dropna(subset=['type_of_traveller', 'overall_rating']).copy()\n",
    "\n",
    "# Consolidar categorías similares\n",
    "traveler_mapping = {\n",
    "    'Family Leisure': 'Family',\n",
    "    'Couple Leisure': 'Couple', \n",
    "    'Solo Leisure': 'Solo',\n",
    "    'Business': 'Business'\n",
    "}\n",
    "traveler_data['traveler_type'] = traveler_data['type_of_traveller'].map(traveler_mapping)\n",
    "traveler_data = traveler_data.dropna(subset=['traveler_type'])\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Gráfico 1: Distribución de ratings por tipo de viajero\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(data=traveler_data, x='traveler_type', y='overall_rating', \n",
    "            order=['Family', 'Couple', 'Solo', 'Business'])\n",
    "plt.title('Distribución de Ratings por Tipo de Viajero', fontweight='bold')\n",
    "plt.xlabel('Tipo de Viajero')\n",
    "plt.ylabel('Overall Rating')\n",
    "\n",
    "# Gráfico 2: Rating promedio por tipo de viajero\n",
    "plt.subplot(2, 2, 2)\n",
    "traveler_stats = traveler_data.groupby('traveler_type').agg({\n",
    "    'overall_rating': ['mean', 'count', 'std'],\n",
    "    'value_for_money': 'mean',\n",
    "    'recommended': lambda x: (x == 'yes').mean()\n",
    "}).round(3)\n",
    "\n",
    "traveler_stats.columns = ['rating_mean', 'count', 'rating_std', 'value_mean', 'recommend_rate']\n",
    "traveler_stats = traveler_stats.sort_values('rating_mean')\n",
    "\n",
    "colors = ['red' if idx in ['Family', 'Couple'] else 'skyblue' for idx in traveler_stats.index]\n",
    "plt.bar(traveler_stats.index, traveler_stats['rating_mean'], \n",
    "        color=colors, alpha=0.7, yerr=traveler_stats['rating_std'], capsize=5)\n",
    "plt.title('Rating Promedio por Tipo de Viajero', fontweight='bold')\n",
    "plt.ylabel('Overall Rating Promedio')\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for i, (idx, row) in enumerate(traveler_stats.iterrows()):\n",
    "    plt.text(i, row['rating_mean'] + 0.1, f'{row[\"rating_mean\"]:.2f}', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Gráfico 3: Tasa de recomendación por tipo de viajero\n",
    "plt.subplot(2, 2, 3)\n",
    "colors_rec = ['red' if idx in ['Family', 'Couple'] else 'skyblue' for idx in traveler_stats.index]\n",
    "plt.bar(traveler_stats.index, traveler_stats['recommend_rate'] * 100, \n",
    "        color=colors_rec, alpha=0.7)\n",
    "plt.title('Tasa de Recomendación por Tipo de Viajero', fontweight='bold')\n",
    "plt.ylabel('Tasa de Recomendación (%)')\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for i, (idx, row) in enumerate(traveler_stats.iterrows()):\n",
    "    plt.text(i, row['recommend_rate'] * 100 + 1, f'{row[\"recommend_rate\"]*100:.1f}%', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Gráfico 4: Análisis de value_for_money por segmento\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(data=traveler_data, x='traveler_type', y='value_for_money',\n",
    "            order=['Family', 'Couple', 'Solo', 'Business'])\n",
    "plt.title('Percepción de Value for Money por Segmento', fontweight='bold')\n",
    "plt.xlabel('Tipo de Viajero')\n",
    "plt.ylabel('Value for Money Rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis estadístico Hipótesis 3\n",
    "print(\"\\nANÁLISIS ESTADÍSTICO HIPÓTESIS 3\")\n",
    "print(\"Estadísticas por tipo de viajero:\")\n",
    "print(traveler_stats)\n",
    "\n",
    "# Test ANOVA para diferencias entre grupos\n",
    "groups = [traveler_data[traveler_data['traveler_type'] == traveler]['overall_rating'] \n",
    "          for traveler in ['Family', 'Couple', 'Solo', 'Business']]\n",
    "\n",
    "f_stat, p_val_anova = f_oneway(*groups)\n",
    "print(f\"\\nTest ANOVA - F-statistic: {f_stat:.3f}, p-value: {p_val_anova:.6f}\")\n",
    "\n",
    "# Comparación específica Family vs Business\n",
    "from scipy.stats import ttest_ind\n",
    "family_ratings = traveler_data[traveler_data['traveler_type'] == 'Family']['overall_rating']\n",
    "business_ratings = traveler_data[traveler_data['traveler_type'] == 'Business']['overall_rating']\n",
    "t_stat, p_val_ttest = ttest_ind(family_ratings, business_ratings, equal_var=False)\n",
    "print(f\"Test t (Family vs Business): t-statistic = {t_stat:.3f}, p-value = {p_val_ttest:.6f}\")\n",
    "\n",
    "print(f\"\\nDiferencia promedio Family-Business: {family_ratings.mean() - business_ratings.mean():.3f}\")\n",
    "\n",
    "\n",
    "print(\"\\n3. HIPÓTESIS DE VULNERABILIDAD DEL SEGMENTO:\")\n",
    "if family_ratings.mean() < business_ratings.mean() and p_val_ttest < 0.05:\n",
    "    print(\"CONFIRMADA: Familias muestran significativamente menor satisfacción\")\n",
    "else:\n",
    "    print(\"NO CONFIRMADA: No hay diferencia significativa entre segmentos\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1de6c8",
   "metadata": {},
   "source": [
    "¡Excelente! Con los resultados reales, aquí están las conclusiones corregidas:\n",
    "\n",
    "## **CONCLUSIÓN HIPÓTESIS 2: \"Hipótesis Temporal\"**\n",
    "\n",
    "### **SATISFACCIÓN HA DETERIORADO**\n",
    "\n",
    "**Hallazgos Críticos:**\n",
    "\n",
    "1. **Tendencia de Deterioro Significativo**\n",
    "   - La satisfacción ha mostrado un **deterioro estadísticamente significativo** en el tiempo\n",
    "   - Contradice la percepción inicial de mejora moderada\n",
    "   - Indica **problemas estructurales o operativos** persistentes\n",
    "\n",
    "2. **Implicaciones Alarmantes**\n",
    "   - El deterioro temporal sugiere que las **iniciativas de mejora no han sido efectivas**\n",
    "   - Posible **erosión de la ventaja competitiva** basada en precio\n",
    "   - **Aumento de expectativas del consumidor** no satisfechas\n",
    "\n",
    "3. **Urgencia de Intervención**\n",
    "   - Necesidad de **revisión profunda** de operaciones y servicio\n",
    "   - Posible **fatiga de marca** en el modelo low-cost\n",
    "   - **Brecha creciente** vs competidores que mejoran servicio\n",
    "\n",
    "---\n",
    "\n",
    "## **\"Hipótesis de Vulnerabilidad del Segmento de Pasajeros\"**\n",
    "\n",
    "### **NO CONFIRMADA - PATRÓN DIFERENTE AL ESPERADO**\n",
    "\n",
    "**Hallazgos Contrarios a la Hipótesis:**\n",
    "\n",
    "1. **Jerarquía Invertida de Satisfacción**\n",
    "   ```\n",
    "   Solo Leisure (4.46) > Couple Leisure (4.42) > Business (3.93) > Family (3.64)\n",
    "   ```\n",
    "\n",
    "2. **Resultados Estadísticos Clave:**\n",
    "   - **ANOVA significativo** (p = 0.0017):  **HAY diferencias entre grupos**\n",
    "   - **Test t Family vs Business NO significativo** (p = 0.421): **NO hay diferencia específica entre estos dos grupos**\n",
    "   - **Las diferencias reales** están entre Solo/Couple vs Family/Business\n",
    "\n",
    "3. **Nuevos Insights Reveladores:**\n",
    "\n",
    "   **Segmento MÁS Satisfecho: SOLO LEISURE (4.46)**\n",
    "   - Viajeros individuales son los **más contentos** con Ryanair\n",
    "   - Probablemente valoran **autonomía, precio, flexibilidad**\n",
    "   - **Segmento objetivo ideal** - alto satisfaction + volumen\n",
    "\n",
    "   **Segmento ESTABLE: COUPLE LEISURE (4.42)**\n",
    "   - Segundos más satisfechos\n",
    "   - Balance entre experiencia compartida y pragmatismo\n",
    "   - **Base sólida** del negocio\n",
    "\n",
    "   **Segmentos PROBLEMÁTICOS:**\n",
    "   - **BUSINESS (3.93)**: Expectativas de servicio no cumplidas\n",
    "   - **FAMILY (3.64)**: Desafíos logísticos y de espacio\n",
    "\n",
    "4. **Análisis de la NO Confirmación:**\n",
    "   - La hipótesis original **sobrestimó** la vulnerabilidad familiar vs business\n",
    "   - **Ambos segmentos (Family y Business)** muestran baja satisfacción\n",
    "   - El **verdadero diferenciador** es viajero individual vs grupal/corporativo\n",
    "\n",
    "**NUEVA HIPÓTESIS EMERGENTE:**\n",
    "**\"Los viajeros individuales (Solo/Couple Leisure) son significativamente más satisfechos que los viajeros con restricciones grupales/corporativas (Family/Business)\"**\n",
    "\n",
    "**Implicaciones Estratégicas Corregidas:**\n",
    "\n",
    "1. **Reenfocar Esfuerzos de Mejora:**\n",
    "   - Priorizar **Business Travelers** (brecha grande vs expectativas)\n",
    "   - **Revisar políticas familiares** (aunque no son los únicos problemáticos)\n",
    "\n",
    "2. **Fortalecer Ventaja Competitiva:**\n",
    "   - **Capitalizar alta satisfacción** en segmento Solo Leisure\n",
    "   - Desarrollar **ofertas específicas** para viajeros individuales\n",
    "\n",
    "3. **Comunicación Segmentada:**\n",
    "   - Mensajes diferentes para **viajeros individuales** vs **grupos/empresas**\n",
    "   - Gestionar expectativas para **segmentos corporativos**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60717b35",
   "metadata": {},
   "source": [
    "*Importante* : No tengo datos numericos, por lo tanto la busqueda de outliers no es necesario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8c071c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31afbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuración\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Preparar datos\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Seleccionar variables numéricas para análisis de correlación\n",
    "numeric_columns = [\n",
    "    'overall_rating', 'seat_comfort', 'cabin_staff_service', \n",
    "    'food_&_beverages', 'ground_service', 'value_for_money',\n",
    "    'inflight_entertainment', 'wifi_&_connectivity'\n",
    "]\n",
    "\n",
    "# Filtrar solo las columnas que existen en el dataset\n",
    "available_columns = [col for col in numeric_columns if col in df_clean.columns]\n",
    "df_corr = df_clean[available_columns].copy()\n",
    "\n",
    "print(\"ANÁLISIS DE COLINEALIDAD Y CORRELACIÓN\")\n",
    "print(f\"Variables analizadas: {available_columns}\")\n",
    "print(f\"Tamaño de muestra: {len(df_corr)} registros\\n\")\n",
    "\n",
    "# Calcular matriz de correlación\n",
    "correlation_matrix = df_corr.corr().round(3)\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4148ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 14))\n",
    "\n",
    "# Heatmap 1: Matriz de correlación completa\n",
    "plt.subplot(2, 2, 1)\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='RdBu_r', center=0,\n",
    "                     square=True, fmt='.3f', mask=mask, \n",
    "                     cbar_kws={'shrink': 0.8}, annot_kws={'size': 10})\n",
    "plt.title('MATRIZ DE CORRELACIÓN - TODAS LAS VARIABLES\\n(Pearson Correlation)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Heatmap 2: Correlaciones solo con overall_rating\n",
    "plt.subplot(2, 2, 2)\n",
    "corr_with_target = correlation_matrix[['overall_rating']].drop('overall_rating')\n",
    "corr_with_target_sorted = corr_with_target.sort_values('overall_rating', ascending=False)\n",
    "\n",
    "sns.heatmap(corr_with_target_sorted, annot=True, cmap='viridis', \n",
    "            fmt='.3f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('CORRELACIÓN CON OVERALL_RATING\\n(Ordenado por importancia)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Variables')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "\n",
    "# Crear matriz de colinealidad (correlaciones entre predictores, excluyendo target)\n",
    "predictors_matrix = correlation_matrix.drop('overall_rating').drop('overall_rating', axis=1)\n",
    "\n",
    "# Resaltar correlaciones altas entre predictores (potencial colinealidad)\n",
    "high_collinearity_mask = (np.abs(predictors_matrix) > 0.7) & (np.abs(predictors_matrix) < 1.0)\n",
    "\n",
    "sns.heatmap(predictors_matrix, annot=True, cmap='YlOrRd', \n",
    "            fmt='.3f', cbar_kws={'shrink': 0.8},\n",
    "            mask=np.eye(len(predictors_matrix)))  # Mask diagonal\n",
    "plt.title('COLINEALIDAD ENTRE PREDICTORES\\n(Correlaciones > 0.7 resaltadas)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a1703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar recommended a numérico\n",
    "df['recommended_numeric'] = df['recommended'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Seleccionar solo columnas numéricas para el análisis de correlación\n",
    "numeric_columns = [\n",
    "    'overall_rating', 'seat_comfort', 'cabin_staff_service', \n",
    "    'food_&_beverages', 'ground_service', 'value_for_money', \n",
    "    'inflight_entertainment', 'wifi_&_connectivity', 'recommended_numeric'\n",
    "]\n",
    "\n",
    "# Calcular matriz de correlación\n",
    "correlation_matrix = df[numeric_columns].corr()\n",
    "\n",
    "# Obtener correlaciones con recommended\n",
    "recommended_correlations = correlation_matrix['recommended_numeric'].sort_values(ascending=False)\n",
    "\n",
    "print(\"CORRELACIONES CON RECOMMENDED (ordenadas por importancia):\")\n",
    "for feature, corr in recommended_correlations.items():\n",
    "    if feature != 'recommended_numeric':\n",
    "        print(f\"{feature:25} : {corr:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# También mostrar correlación entre features para detectar multicolinealidad\n",
    "print(\"MATRIZ DE CORRELACIÓN COMPLETA:\")\n",
    "correlation_matrix.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "\n",
    "# Transformar recommended a numérico\n",
    "df['recommended_numeric'] = df['recommended'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Seleccionar solo columnas numéricas para el análisis de correlación\n",
    "numeric_columns = [\n",
    "    'overall_rating', 'seat_comfort', 'cabin_staff_service', \n",
    "    'food_&_beverages', 'ground_service', 'value_for_money', \n",
    "    'inflight_entertainment', 'wifi_&_connectivity', 'recommended_numeric'\n",
    "]\n",
    "\n",
    "# Calcular matriz de correlación\n",
    "correlation_matrix = df[numeric_columns].corr()\n",
    "\n",
    "# Crear máscara para el triángulo superior\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Crear el heatmap\n",
    "plt.figure(figsize=(16, 9))\n",
    "heatmap = sns.heatmap(correlation_matrix, \n",
    "                    mask=mask,\n",
    "                    annot=True, \n",
    "                    cmap='RdBu_r', \n",
    "                    center=0,\n",
    "                    fmt='.3f',\n",
    "                    square=True,\n",
    "                    cbar_kws={'shrink': 0.8})\n",
    "\n",
    "# Mejorar el título y formato\n",
    "plt.title('MATRIZ DE CORRELACIÓN - PREDICCIÓN DE RECOMENDACIÓN', \n",
    "         fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Resaltar la fila de recommended\n",
    "recommended_idx = list(correlation_matrix.columns).index('recommended_numeric')\n",
    "for i, label in enumerate(heatmap.get_yticklabels()):\n",
    "    if i == recommended_idx:\n",
    "        label.set_fontweight('bold')\n",
    "        label.set_color('darkred')\n",
    "\n",
    "for i, label in enumerate(heatmap.get_xticklabels()):\n",
    "    if i == recommended_idx:\n",
    "        label.set_fontweight('bold')\n",
    "        label.set_color('darkred')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap enfocado solo en correlaciones con recommended\n",
    "plt.figure(figsize=(16, 9))\n",
    "recommended_corrs = correlation_matrix['recommended_numeric'].drop('recommended_numeric')\n",
    "recommended_corrs_sorted = recommended_corrs.sort_values(ascending=False)\n",
    "\n",
    "# Crear gráfico de barras para correlaciones\n",
    "colors = ['#2E8B57' if x > 0.5 else '#FF6B6B' if x > 0.3 else '#FFA500' for x in recommended_corrs_sorted]\n",
    "\n",
    "plt.barh(range(len(recommended_corrs_sorted)), recommended_corrs_sorted.values, color=colors)\n",
    "plt.yticks(range(len(recommended_corrs_sorted)), recommended_corrs_sorted.index)\n",
    "plt.xlabel('Coeficiente de Correlación')\n",
    "plt.title('CORRELACIÓN CON RECOMMENDED', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for i, v in enumerate(recommended_corrs_sorted.values):\n",
    "    plt.text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar correlaciones en formato tabla\n",
    "print(\"CORRELACIONES CON RECOMMENDED (ordenadas por importancia):\")\n",
    "\n",
    "for feature, corr in recommended_corrs_sorted.items():\n",
    "    strength = \"MUY FUERTE\" if abs(corr) > 0.7 else \"FUERTE\" if abs(corr) > 0.5 else \"MODERADA\" if abs(corr) > 0.3 else \"DÉBIL\"\n",
    "    print(f\"{feature:25} : {corr:+.4f} ({strength})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70814ab4",
   "metadata": {},
   "source": [
    "# RECOMENDACIÓN CONCLUSIVA - FEATURES PARA MODELO DE ML\n",
    "\n",
    "## **FEATURES PRINCIPALES (IMPLEMENTAR SÍ O SÍ)**\n",
    "\n",
    "### **1. `overall_rating`** \n",
    "- **Correlación: +0.9050** (MUY FUERTE)\n",
    "- **Impacto**: **CRÍTICO** - Explica el 90.5% de la variabilidad en las recomendaciones\n",
    "- **Recomendación**: **INCLUIR COMO FEATURE PRINCIPAL**\n",
    "\n",
    "### **2. `value_for_money`**\n",
    "- **Correlación: +0.8526** (MUY FUERTE)\n",
    "- **Impacto**: **ALTO** - La relación calidad-precio es fundamental\n",
    "- **Recomendación**: **INCLUIR COMO FEATURE ESENCIAL**\n",
    "\n",
    "### **3. `cabin_staff_service`**\n",
    "- **Correlación: +0.7218** (MUY FUERTE)\n",
    "- **Impacto**: **ALTO** - El servicio de la tripulación es clave\n",
    "- **Recomendación**: **INCLUIR COMO FEATURE PRIMARIO**\n",
    "\n",
    "### **4. `seat_comfort`** \n",
    "- **Correlación: +0.6631** (FUERTE)\n",
    "- **Impacto**: **MEDIO-ALTO** - Comodidad física importante\n",
    "- **Recomendación**: **INCLUIR COMO FEATURE SECUNDARIO**\n",
    "\n",
    "---\n",
    "\n",
    "##  **FEATURES OPCIONALES (CONSIDERAR SEGÚN COMPLEJIDAD)**\n",
    "\n",
    "### **5. `food_&_beverages`** & **6. `ground_service`** \n",
    "- **Correlación: ~+0.417** (MODERADA)\n",
    "- **Impacto**: **MEDIO** - Contribuyen pero no son decisivos\n",
    "- **Recomendación**: **INCLUIR SI SE BUSCA MÁXIMA PRECISIÓN**\n",
    "\n",
    "---\n",
    "\n",
    "##  **FEATURES A DESCARTAR**\n",
    "\n",
    "### **7. `inflight_entertainment`** & **8. `wifi_&_connectivity`** \n",
    "- **Correlación: ~-0.19** (DÉBIL/NEGATIVA)\n",
    "- **Impacto**: **INSIGNIFICANTE** - Prácticamente no influyen\n",
    "- **Recomendación**: **EXCLUIR DEL MODELO**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e97b90b",
   "metadata": {},
   "source": [
    "# Modelo de Mahcine Learning para Prediccion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, confusion_matrix, classification_report, \n",
    "                           roc_auc_score, roc_curve)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e0f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar target a numérico\n",
    "df['recommended_numeric'] = df['recommended'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Seleccionar features basado en el análisis de correlación\n",
    "best_features = [\n",
    "    'overall_rating',\n",
    "    'value_for_money', \n",
    "    'cabin_staff_service',\n",
    "    'seat_comfort',\n",
    "    'food_&_beverages',\n",
    "    'ground_service'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28662e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[best_features].copy()\n",
    "y = df['recommended_numeric']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Escalar features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dataset: {X.shape[0]} muestras, {X.shape[1]} features\")\n",
    "print(f\"Distribución target: {y.value_counts().to_dict()}\")\n",
    "print(f\"Train: {X_train.shape[0]} muestras\")\n",
    "print(f\"Test: {X_test.shape[0]} muestras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c1126",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, verbose=False, iterations=100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cbc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar y evaluar modelos\n",
    "print(\"ENTRENANDO Y EVALUANDO MODELOS\")\n",
    "\n",
    "results = {}\n",
    "predictions = {}\n",
    "y_pred_proba_dict = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n Entrenando {name}...\")\n",
    "    \n",
    "    # Usar datos escalados para modelos que lo requieren\n",
    "    if name in ['Logistic Regression', 'XGBoost', 'LightGBM']:\n",
    "        print(f\"Generando data scaled para : {name}\")\n",
    "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "    else:\n",
    "        print(f\"Generando data para : {name}\")\n",
    "        X_tr, X_te = X_train, X_test\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Predecir\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_pred_proba = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Cross validation\n",
    "    cv_scores = cross_val_score(model, X_tr, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Guardar resultados\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'AUC': auc,\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    predictions[name] = y_pred\n",
    "    y_pred_proba_dict[name] = y_pred_proba\n",
    "    \n",
    "    print(f\"{name} completado - Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Mostrar resultados comparativos\n",
    "print(\" RESULTADOS COMPARATIVOS (CON CATBOOST)\")\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(4)\n",
    "results_df_sorted = results_df.sort_values('Accuracy', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42de7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ba7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de métricas\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Gráfico 1: Accuracy\n",
    "results_df_sorted['Accuracy'].plot(kind='bar', ax=axes[0,0], color='skyblue', alpha=0.8)\n",
    "axes[0,0].set_title('Accuracy por Modelo')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(results_df_sorted['Accuracy']):\n",
    "    axes[0,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Gráfico 2: AUC\n",
    "results_df_sorted['AUC'].plot(kind='bar', ax=axes[0,1], color='lightcoral', alpha=0.8)\n",
    "axes[0,1].set_title('AUC Score por Modelo')\n",
    "axes[0,1].set_ylabel('AUC')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(results_df_sorted['AUC']):\n",
    "    axes[0,1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Gráfico 3: F1-Score\n",
    "results_df_sorted['F1-Score'].plot(kind='bar', ax=axes[0,2], color='lightgreen', alpha=0.8)\n",
    "axes[0,2].set_title('F1-Score por Modelo')\n",
    "axes[0,2].set_ylabel('F1-Score')\n",
    "axes[0,2].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(results_df_sorted['F1-Score']):\n",
    "    axes[0,2].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Gráfico 4: Cross Validation\n",
    "cv_means = results_df_sorted['CV Mean']\n",
    "cv_stds = results_df_sorted['CV Std']\n",
    "axes[1,0].bar(cv_means.index, cv_means.values, yerr=cv_stds.values, \n",
    "             capsize=5, alpha=0.7, color='orange')\n",
    "axes[1,0].set_title('Cross Validation Accuracy (5-fold)')\n",
    "axes[1,0].set_ylabel('Accuracy')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(cv_means.values):\n",
    "    axes[1,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Gráfico 5: Matriz de confusión del mejor modelo\n",
    "best_model_name = results_df['Accuracy'].idxmax()\n",
    "best_y_pred = predictions[best_model_name]\n",
    "cm = confusion_matrix(y_test, best_y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1,1],\n",
    "            xticklabels=['No Recomienda', 'Recomienda'],\n",
    "            yticklabels=['No Recomienda', 'Recomienda'])\n",
    "axes[1,1].set_title(f'Matriz de Confusión - {best_model_name}\\nAccuracy: {results_df.loc[best_model_name, \"Accuracy\"]:.3f}')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f233d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas ROC en subplots individuales\n",
    "print(\"CURVAS ROC - TODOS LOS MODELOS\")\n",
    "\n",
    "# Calcular el número de filas y columnas para los subplots\n",
    "n_models = len(models)\n",
    "n_cols = 3  # Puedes ajustar este número según prefieras\n",
    "n_rows = (n_models + n_cols - 1) // n_cols  # División redondeada hacia arriba\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]  # Aplanar el array de axes\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "\n",
    "for i, (name, color) in enumerate(zip(models.keys(), colors)):\n",
    "    if i < len(axes):  # Asegurarse de que no excedemos el número de subplots\n",
    "        # Calcular métricas ROC para el modelo actual\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba_dict[name])\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba_dict[name])\n",
    "        \n",
    "        # Graficar en el subplot individual\n",
    "        axes[i].plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})', \n",
    "                    linewidth=2.5, color=color, alpha=0.8)\n",
    "        axes[i].plot([0, 1], [0, 1], 'k--', alpha=0.5, \n",
    "                    label='Clasificador Aleatorio', linewidth=2)\n",
    "        \n",
    "        # Configurar el subplot individual\n",
    "        axes[i].set_xlabel('Tasa de Falsos Positivos (FPR)', fontsize=10)\n",
    "        axes[i].set_ylabel('Tasa de Verdaderos Positivos (TPR)', fontsize=10)\n",
    "        axes[i].set_title(f'Curva ROC - {name}', fontsize=12, fontweight='bold')\n",
    "        axes[i].legend(fontsize=9)\n",
    "        axes[i].grid(alpha=0.3)\n",
    "        axes[i].set_xlim([0.0, 1.0])\n",
    "        axes[i].set_ylim([0.0, 1.05])\n",
    "\n",
    "# Ocultar los subplots vacíos si los hay\n",
    "for i in range(len(models), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 IMPORTANCIA DE FEATURES - COMPARATIVO\n",
    "print(\"IMPORTANCIA DE FEATURES - COMPARATIVO\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "all_models = ['Random Forest', 'XGBoost', 'LightGBM', 'CatBoost', 'Logistic Regression']\n",
    "\n",
    "for i, model_name in enumerate(all_models):\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Determinar si es modelo ensemble (usa feature_importances_) o lineal (usa coef_)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Para modelos ensemble\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': best_features,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=True)\n",
    "        \n",
    "        title = f'Feature Importance - {model_name}'\n",
    "        xlabel = 'Importancia'\n",
    "        \n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # Para Logistic Regression - usar valores absolutos de coeficientes\n",
    "        importance_values = np.abs(model.coef_[0])\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': best_features,\n",
    "            'importance': importance_values\n",
    "        }).sort_values('importance', ascending=True)\n",
    "        \n",
    "        title = f'Coeficientes (abs) - {model_name}'\n",
    "        xlabel = '|Coeficiente|'\n",
    "    \n",
    "    # Crear el gráfico\n",
    "    row, col = i//3, i%3\n",
    "    ax = axes[row, col]\n",
    "    bars = ax.barh(feature_importance['feature'], feature_importance['importance'], \n",
    "                  color=plt.cm.viridis(np.linspace(0, 1, len(best_features))))\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.set_xlabel(xlabel)\n",
    "    \n",
    "    # Añadir valores en las barras\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "               f'{width:.3f}', ha='left', va='center', fontsize=8)\n",
    "\n",
    "# Feature importance promedio (solo para modelos con feature_importances_)\n",
    "ensemble_models = ['Random Forest', 'XGBoost', 'LightGBM', 'CatBoost']\n",
    "avg_importance = pd.DataFrame({'feature': best_features})\n",
    "\n",
    "for model_name in all_models:\n",
    "    model = models[model_name]\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Para modelos ensemble\n",
    "        avg_importance[model_name] = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # Para Logistic Regression - normalizar coeficientes para comparar\n",
    "        coef_abs = np.abs(model.coef_[0])\n",
    "        # Normalizar para que sumen 1 (similar a feature_importances_)\n",
    "        coef_normalized = coef_abs / coef_abs.sum()\n",
    "        avg_importance[model_name] = coef_normalized\n",
    "\n",
    "# Calcular promedio de todos los modelos\n",
    "avg_importance['Average'] = avg_importance[all_models].mean(axis=1)\n",
    "avg_importance = avg_importance.sort_values('Average', ascending=True)\n",
    "\n",
    "# Gráfico de promedio\n",
    "axes[1, 2].barh(avg_importance['feature'], avg_importance['Average'],\n",
    "               color=plt.cm.plasma(np.linspace(0, 1, len(best_features))))\n",
    "axes[1, 2].set_title('Importancia Promedio - Todos los Modelos', fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Importancia Promedio')\n",
    "\n",
    "# Añadir valores en las barras del promedio\n",
    "bars_avg = axes[1, 2].containers[0]\n",
    "for bar in bars_avg:\n",
    "    width = bar.get_width()\n",
    "    axes[1, 2].text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{width:.3f}', ha='left', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac9c11",
   "metadata": {},
   "source": [
    "# Conclusiones finales:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb7da16",
   "metadata": {},
   "source": [
    "### **Mejor Opción: CatBoost**\n",
    "\n",
    "**Razones principales:**\n",
    "\n",
    "1. **Mayor poder discriminativo**: CatBoost muestra el rango más amplio de importancias (0.10-0.18) comparado con otros modelos, indicando una mejor capacidad para distinguir entre características importantes y menos importantes.\n",
    "\n",
    "2. **Consistencia en características clave**:\n",
    "   - `overall_raising` y `value_for_promy` son consistentemente las dos características más importantes en todos los modelos\n",
    "   - CatBoost mantiene este patrón pero con mejor separación entre características\n",
    "\n",
    "3. **Estabilidad**: Las importancias en CatBoost están mejor distribuidas sin la superposición excesiva que se observa en algunos otros modelos.\n",
    "\n",
    "### **Ranking de Modelos:**\n",
    "\n",
    "1. **CatBoost** - Mejor balance y discriminación\n",
    "2. **Random Forest** - Buen rango de importancias pero menos refinado\n",
    "3. **XGBoost** - Similar a CatBoost pero con importancias más concentradas\n",
    "4. **Logistic Regression** - Patrón similar a CatBoost pero posiblemente menos robusto\n",
    "\n",
    "### **Características Más Importantes (Consistentes en todos los modelos):**\n",
    "\n",
    "1. **overall_raising** - La más importante en todos los modelos\n",
    "2. **value_for_promy** - Segunda en importancia consistentemente\n",
    "3. **ground_service** - Tercera posición en la mayoría de modelos\n",
    "4. **cabin_stuff_service** - Cuarta en importancia general\n",
    "\n",
    "### **Recomendación Final:**\n",
    "\n",
    "**CatBoost** sería la mejor opción porque:\n",
    "- Proporciona la mejor separación entre características importantes y menos importantes\n",
    "- Mantiene consistencia con los patrones encontrados en otros modelos\n",
    "- Es conocido por su buen rendimiento con datos categóricos y su robustez frente al overfitting\n",
    "- Las importancias bien definidas facilitan la interpretación del modelo para toma de decisiones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "77695-data-science-i-flex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
